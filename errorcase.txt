With python code as below:

    def alphabeta(self, game, depth, alpha=float("-inf"), beta=float("inf")):
        """
        if self.time_left() < self.TIMER_THRESHOLD:
            raise SearchTimeout()

        v = self.max_value(game, depth, alpha, beta)
        legal_moves = game.get_legal_moves()
        move = (-1, -1)

        for m in legal_moves:
            if self.min_value(game.forecast_move(m), depth - 1, alpha, beta) == v:
                move = m 
                break

        return move
        """
        legal_moves = game.get_legal_moves()
        if legal_moves:
            _, move = max([(self.min_value(game.forecast_move(m), depth - 1, alpha, beta), m) for m in legal_moves])
        else:
            move = (-1, -1)
        return move

    def min_value(self, game, depth, alpha=float("-inf"), beta=float("inf")):
        if self.time_left() < self.TIMER_THRESHOLD:
            raise SearchTimeout()

        if depth == 0:
            return self.score(game, self)
        
        legal_moves = game.get_legal_moves()
        v = float("inf")

        for m in legal_moves:
            v =  min(v, self.max_value(game.forecast_move(m), depth -1, alpha, beta))
            if v <= alpha:
                return v
            beta = min(beta, v)
        
        return v

    def max_value(self, game, depth, alpha=float("-inf"), beta=float("inf")):
        if self.time_left() < self.TIMER_THRESHOLD:
            raise SearchTimeout()

        if depth == 0 :
            return self.score(game, self)
        
        legal_moves = game.get_legal_moves()
        v = float("-inf")

        for m in legal_moves:
            v =  max(v, self.min_value(game.forecast_move(m), depth -1, alpha, beta))
            if v >= beta:
                return v
            alpha = max(alpha, v)
        
        return v


Failed Test: 7. Test functionality of AlphaBetaPlayer.alphabeta()
----------------------------------------------------------------------
AssertionError: Failed to cut off search -- expanded too many nodes. (i.e., your agent did not prune at this node, but a correct alpha beta search did prune at this node when following the same expansion order that your agent followed.)
Alpha: 3.0
Beta: 3.0
Game tree evaluation order:
[(0, 4), (0, 6), (1, 3)]
[(0, 3), (0, 5)]

Nodes are shown with each layer sorted in the order the nodes were expanded
during search.  All nodes in each successive layer are children of the
furthest-right node in the parent layer above it.

Test Case Details:
------------------
Heuristic: open_move_score
Depth limit: 2
Initial Board State:
     0   1   2   3   4   5   6   7   8
0  |   |   |   |   |   |   |   |   |   |
1  |   |   |   |   |   |   |   |   |   |
2  |   |   |   |   | 2 | 1 |   |   |   |
3  |   |   |   |   | - |   | - | - |   |
4  |   |   |   | - | - |   |   |   |   |
5  |   |   |   | - | - | - | - |   |   |
6  |   |   | - | - | - | - | - |   |   |
7  |   |   |   |   |   |   |   |   |   |
8  |   |   |   |   |   |   |   |   |   |

game._board_state:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 38, 47]

Failed Test: 7. Test functionality of AlphaBetaPlayer.alphabeta()
----------------------------------------------------------------------
AssertionError: Failed to cut off search -- expanded too many nodes. (i.e., your agent did not prune at this node, but a correct alpha beta search did prune at this node when following the same expansion order that your agent followed.)
Alpha: 5.0
Beta: 5.0
Game tree evaluation order:
[(3, 3), (3, 5)]
[(2, 3)]

Nodes are shown with each layer sorted in the order the nodes were expanded
during search.  All nodes in each successive layer are children of the
furthest-right node in the parent layer above it.

Test Case Details:
------------------
Heuristic: open_move_score
Depth limit: 2
Initial Board State:
     0   1   2   3   4   5   6   7   8
0  |   |   |   |   |   |   |   |   |   |
1  |   |   |   |   |   |   |   |   |   |
2  |   |   | - |   |   |   |   |   |   |
3  |   |   |   |   | - |   |   |   |   |
4  |   |   | - |   | 2 | - |   |   |   |
5  |   |   |   |   | 1 |   | - |   |   |
6  |   |   |   | - | - |   | - |   |   |
7  |   |   |   |   |   |   |   |   |   |
8  |   |   |   |   |   |   |   |   |   |

game._board_state:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 40, 41]

Failed Test: 7. Test functionality of AlphaBetaPlayer.alphabeta()
----------------------------------------------------------------------
AssertionError: Failed to cut off search -- expanded too many nodes. (i.e., your agent did not prune at this node, but a correct alpha beta search did prune at this node when following the same expansion order that your agent followed.)
Alpha: 2.0
Beta: 2.0
Game tree evaluation order:
[(0, 5), (0, 7)]
[(3, 6)]

Nodes are shown with each layer sorted in the order the nodes were expanded
during search.  All nodes in each successive layer are children of the
furthest-right node in the parent layer above it.

Test Case Details:
------------------
Heuristic: open_move_score
Depth limit: 2
Initial Board State:
     0   1   2   3   4   5   6   7   8
0  |   |   |   |   |   |   |   |   |   |
1  |   |   |   |   |   |   |   |   |   |
2  |   |   | - |   | - |   | 1 |   |   |
3  |   |   | - | - | - |   |   |   |   |
4  |   |   |   | - |   | - |   |   |   |
5  |   |   | - |   | - | 2 |   |   |   |
6  |   |   |   |   |   |   | - |   |   |
7  |   |   |   |   |   |   |   |   |   |
8  |   |   |   |   |   |   |   |   |   |

game._board_state:
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 50, 56]

